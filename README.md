# GridForecast-LSTM
GridForecast-LSTM introduces a high-precision, utility-oriented voltage forecasting framework tailored for hourahead predictions in smart distribution systems. By integrating historical voltage data, environmental parameters, and load variability into a Long Short-Term Memory (LSTM) architecture.

README GridForecast-LSTM


GridForecast-LSTM: A Utility-Centric Model for
Hour-Ahead Voltage Forecasting
Adnan Haider Zaidi, Member, IEEE


GitHub Repository (Mandatory Link)
‚Ä¢	Repository URL:   https://github.com/adnanzaidi548/GridForecast-LSTM


GridForecast-LSTM introduces a high-precision,
utility-oriented voltage forecasting framework tailored for hourahead
predictions in smart distribution systems. By integrating
historical voltage data, environmental parameters, and load
variability into a Long Short-Term Memory (LSTM) architecture,
this model achieves dynamic adaptability and robust
temporal learning. The novelty lies in its ability to self-tune
under grid state fluctuations, accommodating distributed energy
resources (DERs), electric vehicle load surges, and weatherinduced
instability. Addressing the critical need for predictive
voltage control, GridForecast-LSTM enhances grid resilience,
minimizes transformer stress, and reduces blackout probabilities.
Future applications include real-time integration into SCADA
and digital twin environments for live utility feedback loops

Source of the Dataset
‚Ä¢	Primary Dataset:
Publicly available from the Ontario Independent Electricity System Operator (IESO) Smart Metering Entity (SME) repository.
‚Ä¢	Time Resolution:
Hourly data over one year (i.e., 8760 data points).
‚Ä¢	Granularity:
Residential substations and distribution feeders.

Each input vector at time t, denoted xt ‚àà ‚Ñù‚Åµ, includes:
‚Ä¢	Vt: Voltage [Volts]
‚Ä¢	Pt: Load [kW]
‚Ä¢	Tt: Temperature [¬∞C]
‚Ä¢	Ht: Humidity [%]
‚Ä¢	Wt: Wind speed [km/h]


A. Cleaning & Noise Removal
‚Ä¢	Imputation for missing values via:
o	Linear interpolation
o	Temporal K-Nearest Neighbors (KNN)
‚Ä¢	Outlier detection using IQR thresholds or Z-score method
B. Normalization
‚Ä¢	Min-Max Scaling across all features:
X‚Ä≤=X‚àíXminXmax‚àíXminX' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}} 
C. Temporal Structuring
‚Ä¢	Sliding Window with:
o	Input sequence length (lookback) = 24 hours
o	Prediction horizon = 1 hour ahead
________________________________________
üß† 4. Target Output
‚Ä¢	Forecasted voltage value (VÃÇt+1) at the feeder level, 1 hour ahead.
________________________________________
üõ†Ô∏è 5. Jupyter Notebook Code Requirements
Modules to be included:
‚Ä¢	pandas, numpy, matplotlib, scikit-learn for preprocessing & visualization
‚Ä¢	tensorflow and keras for LSTM model training
‚Ä¢	optuna or kerastuner for hyperparameter tuning
‚Ä¢	shap or lime for interpretability
Notebook Sections:
Step	Notebook Cell Purpose
1.	Data loading from IESO CSV / URL
2.	Data cleaning, imputation, noise filtering
3.	Feature engineering (Voltage, Load, Temp, etc.)
4.	Sliding window transformation
5.	Train/test/validation split (70/15/15)
6.	Min-max normalization
7.	LSTM model definition (2 or 3 hidden layers, dropout, attention)
8.	Model training and loss tracking
9.	Performance evaluation: MAE, RMSE, MAPE, R¬≤
10.	Comparison plots: predicted vs actual voltage
11.	Export model checkpoints and predictions
12.	GitHub integration and documentation for reproducibility
________________________________________
üìà 6. Experimental Setup Summary
‚Ä¢	Hardware: NVIDIA GPU (e.g., Tesla T4 / RTX 3090 / A100)
‚Ä¢	Software: Python 3.10 / 3.11, TensorFlow 2.12/2.14
‚Ä¢	Batch Size: 64
‚Ä¢	Epochs: 100‚Äì200
‚Ä¢	Optimizer: Adam (lr=0.001)
‚Ä¢	Loss Function: MSE or MAE
‚Ä¢	Dropout: 0.2‚Äì0.3
‚Ä¢	Validation Split: 15%

GitHub Repository (Mandatory Link)
‚Ä¢	Repository URL:


https://github.com/adnanzaidi548/GridForecast-LSTM


‚Ä¢	Shape of X_train, y_train, X_val, X_test
‚Ä¢	Example input window: [[Vt, Tt, Ht, Wt, Pt] for 24 timesteps]
‚Ä¢	Visualizations:
o	Actual vs Predicted Voltage Plot (24‚Äì48 hour span)
o	SHAP summary plot (feature importance)
o	Training loss & validation loss curves
‚Ä¢	Final results (e.g., MAE = 0.96, RMSE = 1.12, MAPE = 1.83%)



